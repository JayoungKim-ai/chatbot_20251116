from datetime import datetime
import time
import re
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs

import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate

from dotenv import load_dotenv


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(".env")

# í´ë” ì„¤ì •
DATA_DIR = "./naver_finance_news"
VECTORSTORE_DIR = "faiss_index"


# ================================================
# 1. CSV íŒŒì¼ â†’ Document ë³€í™˜ + Chunk ë‚˜ëˆ„ê¸°
# ================================================
def load_csv_and_split(date_str):
    csv_path = os.path.join(DATA_DIR, f"{date_str}.csv")

    if not os.path.exists(csv_path):
        return None, f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

    df = pd.read_csv(csv_path)

    docs = []
    for _, row in df.iterrows():
        subject = str(row["subject"])
        content = str(row["content"])

        # ì œëª© + ë³¸ë¬¸ìœ¼ë¡œ Document êµ¬ì„±
        text = f"[ì œëª©] {subject}\n\n{content}"

        docs.append(
            Document(
                page_content=text,
                metadata={
                    "subject": subject,
                    "content_length": len(content)
                }
            )
        )

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50
    )

    return splitter.split_documents(docs), None



# ================================================
# 2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (Batch ì ìš©)
# ================================================
def create_vectorstore(docs):
    embeddings = OpenAIEmbeddings()
    batch_size = 100

    vectordb = None

    progress = st.progress(0)
    status = st.empty()

    total = (len(docs) + batch_size - 1) // batch_size

    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        now_batch = i // batch_size + 1

        progress.progress(now_batch / total)
        status.text(f"{now_batch}/{total} ë°°ì¹˜ ì„ë² ë”© ì¤‘...")

        if vectordb is None:
            vectordb = FAISS.from_documents(batch, embeddings)
        else:
            temp_db = FAISS.from_documents(batch, embeddings)
            vectordb.merge_from(temp_db)

    vectordb.save_local(VECTORSTORE_DIR)

    progress.empty()
    status.empty()

    return vectordb


# ================================================
# 3. ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
# ================================================
def load_vectorstore():
    embeddings = OpenAIEmbeddings()
    if os.path.exists(VECTORSTORE_DIR):
        return FAISS.load_local(
            VECTORSTORE_DIR,
            embeddings,
            allow_dangerous_deserialization=True
        )
    return None


# ================================================
# 4. RAG ì²´ì¸ ìƒì„±
# ================================================
def build_rag_chain(vectordb):
    retriever = vectordb.as_retriever()

    prompt = ChatPromptTemplate.from_template("""
    ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {question}

    ì°¸ê³  ë¬¸ì„œ:
    {context}
    """)

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    chain = (
        {
            "context": RunnableLambda(lambda x: x["question"]) | retriever,
            "question": RunnableLambda(lambda x: x["question"])
        }
        | prompt
        | llm
    )
    return chain

# ================================================
# 5. ë‰´ìŠ¤ 100ì ìš”ì•½
# ================================================
def summarize_news(df):
    """ì „ì²´ ë‰´ìŠ¤ë¥¼ 100ì ë‚´ì™¸ë¡œ ìš”ì•½"""
    all_text = " ".join(df["content"].astype(str).tolist())

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    summary_prompt = f"""
    ì•„ë˜ëŠ” ë„¤ì´ë²„ ì¦ê¶Œë‰´ìŠ¤ ì „ì²´ ë‚´ìš©ì…ë‹ˆë‹¤.
    í•µì‹¬ ë‚´ìš©ì„ 100ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

    ë‚´ìš©:
    {all_text[:10000]}  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (í† í° ë³´í˜¸)
    """

    response = llm.invoke([{"role":"user","content": summary_prompt}])
    return response.content


# ================================================
# 6. wordcloud ìƒì„± 
# ================================================
from kiwipiepy import Kiwi

kiwi = Kiwi()


KOREAN_STOPWORDS = set("""
ê°€ ê° ê°„ ê° ê°’ ê²ƒ ê² ê²½ìš° ê²Œ ê²°ê³¼ ê³  ê³³ ê³¼ ê´€ê³„ ê´€ë ¨ ê´€ì‹¬ ê´€í•´ ê±°ì˜ 
ê·¸ë˜ ê·¸ëŸ¬ë‚˜ ê·¸ëŸ¬ë‚˜ ê·¸ë˜ë„ ê·¸ë˜ì„œ ê·¸ë¦¬ê³  ê·¸ëŸ¬ë©´ ê·¸ëŸ° ê·¸ëŸ°ì§€ ê·¸ëŸ¼ ê·¸ë•Œ ê·¸ë•Œë¬¸ì— 
ê·¸ëŸ°ë° ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸€ê³  ê¸°íƒ€ ê·¸ëƒ¥ ê·¸ë‚˜ë§ˆ ê·¸ëŸ¼ì— ë”°ë¼ ê·¸ë¿ì´ë‹¤ ê·¸ë°–ì— ê·¸ì¤‘ ê·¸ë™ì•ˆ 
ë‚˜ ë‚¨ ë„ˆë¬´ ë…„ ë…„ëŒ€ ë‚´ë‚´ ë„· ëˆ„êµ¬ ë‹¤ ë‹¤ì‹œ ë‹¨ ë‹¨ì§€ ëŒ€ ëŒ€ë‹¤ ëŒ€ë¶€ë¶„ ë”ìš± ë”ìš±ë” 
ë”ë”ìš± ë•Œë¬¸ì— ë˜ ë˜ëŠ” ë˜í•œ ë•Œ ë•Œë¡œ ë•Œë§ˆë‹¤ ë“± ë“±ì´ ë”°ë¼ ë˜ëŠ” ë”°ë¦„ë§Œ ë”°ë¦„ì´ë‹¤ 
ë¼ ë¡œ ë¡œì¨ ë¥¼ ëª¨ë“  ë° ë°” ë°”ëŒ ë°˜ ì—¬ëŸ¬ ì—¬ëŸ¬ê°€ì§€ ì—¬ëŸ¬ê°œ ì—¬ëŸ¬ë²ˆ ì—¬ëŸ¬í•´ ì–´ ì–´ë„ 
ì—„ì²­ ì—¬ ì—¬ì „íˆ ì—­ì‹œ ì˜¤ ì˜¤íˆë ¤ ì™€ ì™œ ì™¸ ì™¸ì— ìš” ìš°ë¦¬ ìš°ë¦¬ë‚˜ë¼ ìš°ë¦¬ë“¤ ìš°ë¦¬ê°€ ìœ„ ìœ„í•˜ì—¬ 
ìœ„í•´ ìœ„í•œ ìœ¼ë¡œ ì€ ì´ëŠ” ì´ë²ˆ ì´ë˜ ì´ëŸ¬ ì´ëŸ¬ì´ëŸ¬í•œ ì´ëŸ¬í•œ ì´ëŸ° ì´ë¼ê³  ì´ëŸ¬í•œë° 
ì´ë¦„ ì´í›„ ì´ì™¸ ì´ì™€ ì´ëŸ°ì €ëŸ° ì´ì   ì´ì œ ì¼ ì¼ë‹¨ ì¼ë°˜ ì¼ë°˜ì  ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ ì´ì™¸ 
ì ì „ ì „í˜€ ì „ì²´ ì „ì²´ì  ì „ì²´ì ìœ¼ë¡œ ì œ ì£¼ë³€ ì§€ê¸ˆ ì¦‰ ì§€ë§Œ ì§„ì§œ ì œëŒ€ë¡œ ì¤„ ì¤‘ ì¤‘ì— 
ì§€ ê¸ˆ ê°€ì¥ ê°€ì¥ìœ¼ë¡œ ê°€ì¥ì€ ê°€ì¥ì€ ì˜ ì˜ëª» ì˜ëª»ëœ ì  ì ì–´ë„ ì ì ˆ ì ˆëŒ€ ì ˆëŒ€ë¡œ ì£¼ë¡œ ì§‘ê·¼ì²˜ 
ì²˜ ì²˜ìŒ ì²« ì²«ì§¸ ìµœê·¼ ì°¸ê³  í†µí•´ í†µí•´ í‹€ë¦¼ì—†ì´ í¸ í¬ ì´ìƒ ì´ë˜ì„œ ì´í›„ë¡œ ì´ëŸ°ë“¤ 
ë•Œë¬¸ ë•Œë¬¸ì´ë‹¤ ë”°ë¼ì„œ í•˜ì§€ë§Œ í˜¹ì€ í˜¹ì‹œ í˜¹ ìˆë‹¤ ì—†ë‹¤ ì—†ë‹¤ë©´ ë§ì´ ë§ì€ ë§¤ìš° ë§¤ìš°ë„ ë§¨ 
í•´ì•¼ í•œ í•œë‹¤ í•˜ëŠ” í•˜ëŠ”ë° í•˜ë‚˜ í•˜ë‚˜ì”© í•œë²ˆ í•œë²ˆë„ í•˜ì—¬ í•˜ì—¬ê¸ˆ í•˜ì§€ë§Œ í•˜ê³  í•˜ë©° í•˜ë©´ 
í•˜ëŠ”ë° í•˜ë“¯ í•˜ë“ ì§€ í•˜ê²Œ í•˜ë„ë¡ í•˜ì í•˜ìë§ˆì í•˜ì§„ í˜¹ì‹œ í˜¹ì€ í˜¹ì‹œë‚˜ í˜¹ì€ 
""".split())
MODERN_COLORS = [
    "#0D1B2A",  # deep navy
    "#1B263B",  # navy
    "#415A77",  # blue-gray
    "#778DA9",  # soft gray-blue
    "#E0E1DD"   # washed white
]

def extract_nouns(text: str):
    """Kiwië¡œ ë¬¸ì¥ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ"""
    nouns = []
    for token in kiwi.tokenize(text):
        if token.tag in ["NNG", "NNP"]:   # ì¼ë°˜ëª…ì‚¬/ê³ ìœ ëª…ì‚¬
            nouns.append(token.form)
    return nouns

def modern_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    return MODERN_COLORS[hash(word) % len(MODERN_COLORS)]

def generate_wordcloud(df):
    # 1) ì œëª©(subject)ë§Œ í•©ì¹˜ê¸°
    text = " ".join(df["subject"].astype(str).tolist())

    # 2) íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r"[^ê°€-í£0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    # 3) Kiwië¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
    nouns = extract_nouns(text)

    # 4) ë¶ˆìš©ì–´ ì œê±°
    words = [
        noun for noun in nouns
        if noun not in KOREAN_STOPWORDS and len(noun) > 1
    ]

    cleaned_text = " ".join(words)

    font_path = "C:/Windows/Fonts/malgun.ttf"

    # ğŸ”¥ ê¸°ë³¸ ë””ìì¸ ì›Œë“œí´ë¼ìš°ë“œ
    wc = WordCloud(
        font_path=font_path,
        background_color="white",   # ê¸°ë³¸ ë°°ê²½
        width=900,
        height=500,
        max_words=200,
    ).generate(cleaned_text)

    return wc


# ================================================
# 7. í¬ë¡¤ë§
# ================================================
def crawl_naver_finance_news(date_str):
    page = 1
    article_list = []
    # ğŸ”¥ Streamlit ìƒíƒœ í‘œì‹œ: ì—¬ê¸°ì— í•œ ì¤„ë§Œ ê³„ì† ì—…ë°ì´íŠ¸ë¨
    status = st.empty()
    while True:
        print(f"===== {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ =====")
        status.write(f"ğŸ“¡ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

        url = f"https://finance.naver.com/news/mainnews.naver?date={date_str}&page={page}"

        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select(".block1")

        if not articles:
            break  # ë” ì´ìƒ ë‰´ìŠ¤ ì—†ìœ¼ë©´ ì¢…ë£Œ

        for article in articles:
            subject = article.select_one(".articleSubject").text.strip()
            print(subject)
            status.write(f"ğŸ“° ê¸°ì‚¬ ìˆ˜ì§‘: {subject}")
            # ë§í¬ ìˆ˜ì§‘
            link = article.select_one(".articleSubject>a").attrs["href"]
            parsed = urlparse(link)
            params = parse_qs(parsed.query)
            article_id = params['article_id'][0]
            office_id = params['office_id'][0]
            news_link = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'

            # ë‚´ìš© ìˆ˜ì§‘
            detail_html = requests.get(news_link).text
            detail_soup = BeautifulSoup(detail_html, "html.parser")
            content = detail_soup.select_one("#dic_area").text.strip()

            time.sleep(0.5)

            article_list.append({
                "subject": subject,
                "content": content
            })
        
        if soup.select_one(".pgRR") is None:
            break

        page += 1
        time.sleep(1)

    # ì €ì¥
    df = pd.DataFrame(article_list)
    csv_path = f"./naver_finance_news/{date_str}.csv"
    df.to_csv(csv_path, index=False)

    return csv_path, len(df)




# ================================================
# 8. Streamlit UI
# ================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None


# ----------------------------------------
# ë‚ ì§œ ì„ íƒ UI
# ----------------------------------------
st.set_page_config(page_title="ë„¤ì´ë²„ ì¦ê¶Œë‰´ìŠ¤ RAG ì±—ë´‡", layout="wide")


# --- ì‚¬ì´ë“œë°” ì„¤ì • ---
with st.sidebar:
    st.title("ğŸ“ˆ ë„¤ì´ë²„ ì¦ê¶Œë‰´ìŠ¤ RAG ì±—ë´‡")
    st.header("âš™ï¸ ì„¤ì •")

    selected_date = st.date_input("ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”", value=datetime.today())
    date_str = selected_date.strftime("%Y-%m-%d")

    if st.button("ê¸ˆì¼ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ ğŸš€"):
        csv_path = f"./naver_finance_news/{date_str}.csv"

        # 1) CSV ì—†ìœ¼ë©´ í¬ë¡¤ë§
        if not os.path.exists(csv_path):
            with st.spinner("CSV íŒŒì¼ì´ ì—†ì–´ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤..."):
                csv_path, count = crawl_naver_finance_news(date_str)
            st.success(f"{count}ê°œì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•´ CSVë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")

        # 2) CSV ë¡œë“œ
        df = pd.read_csv(csv_path)
        st.session_state["df"] = df

        # 3) VectorDB ìƒì„±
        split_docs, error = load_csv_and_split(date_str)
        if error:
            st.error(error)
            st.stop()

        with st.spinner("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘..."):
            vectordb = create_vectorstore(split_docs)
            st.session_state.vectordb = vectordb
            st.session_state.rag_chain = build_rag_chain(vectordb)

        # 4) ë‰´ìŠ¤ ìš”ì•½ (50ìë¡œ ë°”ê¿€ ì˜ˆì • â€“ 2ë‹¨ê³„ì—ì„œ ìˆ˜ì •)
        with st.spinner("ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘..."):
            summary = summarize_news(df)
            st.session_state["summary"] = summary

        # 5) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
            wc = generate_wordcloud(df)
            st.session_state["wordcloud"] = wc

            # ğŸ”¥ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë„ ì €ì¥ (4ë‹¨ê³„ì—ì„œ í™œìš©)
            top_keywords = sorted(wc.words_.items(), key=lambda x: x[1], reverse=True)[:10]
            st.session_state["top_keywords"] = [w for w, _ in top_keywords]

        st.success("ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš” ğŸ‘‡")

# ë‚ ì§œ/ìš”ì¼ í‘œì‹œ
weekday_kor = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][selected_date.weekday()]
display_date = selected_date.strftime("%Y-%m-%d")

st.markdown(
    f"""
    <h1 style="margin-top:0.5rem; margin-bottom:1rem;">
        ğŸ“… {display_date} ({weekday_kor}ìš”ì¼)
    </h1>
    """,
    unsafe_allow_html=True
)


if "summary" in st.session_state or "wordcloud" in st.session_state:
    st.markdown("---")
    st.subheader("ğŸ“Š ì˜¤ëŠ˜ì˜ ì‹œì¥ í•œëˆˆì— ë³´ê¸°")

    # ì™¼ìª½: ì›Œë“œí´ë¼ìš°ë“œ, ì˜¤ë¥¸ìª½: ìš”ì•½
    col1, col2 = st.columns([1, 1])  # ì™¼ìª½ ë„“ê²Œ / ì˜¤ë¥¸ìª½ ì¢ê²Œ

    # ì™¼ìª½: ì›Œë“œí´ë¼ìš°ë“œ ì¹´ë“œ
    with col1:
        if "wordcloud" in st.session_state:
            wc = st.session_state["wordcloud"]
            fig, ax = plt.subplots(figsize=(6, 4))  # ğŸ”¥ ì™¼ìª½ì´ ë„“ìœ¼ë‹ˆê¹Œ ì¡°ê¸ˆ ë” í¬ê²Œ
            ax.imshow(wc, interpolation='bilinear')
            ax.axis("off")
            st.pyplot(fig)

            st.markdown("</div>", unsafe_allow_html=True)

    # ì˜¤ë¥¸ìª½: 50ì ìš”ì•½ ì¹´ë“œ
    with col2:
        if "summary" in st.session_state:
            st.markdown(
                f"""
                <div style="
                    padding:1rem 1.25rem;
                    border-radius:1rem;
                    background-color:#F9FAFB;
                    border:1px solid #E5E7EB;
                    box-shadow:0 1px 3px rgba(15,23,42,0.08);
                    font-size:0.95rem;
                    line-height:1.5;
                    height:100%;
                ">
                    <div style="font-size:0.85rem; color:#6B7280; margin-bottom:0.25rem;">
                        ğŸ“Œ ì˜¤ëŠ˜ ì¦ê¶Œë‰´ìŠ¤ 50ì ìš”ì•½
                    </div>
                    <div style="font-weight:500; color:#111827;">
                        {st.session_state["summary"]}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )


# ----------------------------------------
# ğŸ”¥ TOP 10 í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ì§ˆë¬¸ ìƒì„±
# ----------------------------------------
if "top_keywords" in st.session_state:

    st.markdown("### ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ë¶„ì„")

    keywords = st.session_state["top_keywords"][:10]

    btn_refs = []

    # 10ê°œ í‚¤ì›Œë“œë¥¼ 5ê°œì”© ë‚˜ëˆ”
    chunks = [keywords[i:i+5] for i in range(0, len(keywords), 5)]

    for chunk in chunks:
        cols = st.columns(len(chunk))
        for idx, kw in enumerate(chunk):
            with cols[idx]:
                if st.button(f"ã€{kw}ã€‘ ì´ìŠˆ 50ì ìš”ì•½", key=f"kwbtn_{kw}"):
                    btn_refs.append(kw)

    # ë²„íŠ¼ ëˆŒë €ì„ ë•Œ ìë™ ì§ˆë¬¸ ì‹¤í–‰
    if btn_refs:
        auto_question = f"{btn_refs[0]} ê´€ë ¨ ì´ìŠˆë¥¼ 50ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì¤˜."
        st.write(f"**ì§ˆë¬¸ ìë™ ìƒì„±:** {auto_question}")
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = st.session_state.rag_chain.invoke({"question": auto_question})
        st.write(result.content)

    st.markdown("---")


# ----------------------------------------
# ğŸ”¥ ì§ì ‘ ì…ë ¥í•˜ëŠ” ì§ˆë¬¸ UI
# ----------------------------------------
st.subheader("ğŸ” ë‰´ìŠ¤ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ì§ì ‘ ì§ˆë¬¸)")

if st.session_state.rag_chain:
    user_question = st.text_input("ì›í•˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if user_question:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = st.session_state.rag_chain.invoke({"question": user_question})

        st.write("### ğŸ’¬ ë‹µë³€")
        st.write(result.content)
else:
    st.info("ë¨¼ì € ë‚ ì§œë¥¼ ì„ íƒí•˜ê³  ë‰´ìŠ¤ ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

a